{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReference: https://nvidia.github.io/cuda-python/install.html\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reference: https://nvidia.github.io/cuda-python/install.html\n",
    "\"\"\"\n",
    "%pip install cuda-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cuda import cuda, nvrtc\n",
    "import numpy as np\n",
    "\n",
    "def ASSERT_DRV(err):\n",
    "    if isinstance(err, cuda.CUresult):\n",
    "        if err != cuda.CUresult.CUDA_SUCCESS:\n",
    "            raise RuntimeError(\"Cuda Error: {}\".format(err))\n",
    "    elif isinstance(err, nvrtc.nvrtcResult):\n",
    "        if err != nvrtc.nvrtcResult.NVRTC_SUCCESS:\n",
    "            raise RuntimeError(\"Nvrtc Error: {}\".format(err))\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown error type: {}\".format(err))\n",
    "    \n",
    "saxpy = \"\"\"\\\n",
    "extern \"C\" __global__\n",
    "void saxpy(float a, float *x, float *y, float *out, size_t n)\n",
    "{\n",
    " size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    " if (tid < n) {\n",
    "   out[tid] = a * x[tid] + y[tid];\n",
    " }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create program\n",
    "err, prog = nvrtc.nvrtcCreateProgram(str.encode(saxpy), b\"saxpy.cu\", 0, [], [])\n",
    "\n",
    "# Compile program\n",
    "# opts = [b\"--fmad=false\", b\"--gpu-architecture=compute_75\"]\n",
    "opts = [b\"--fmad=false\", b\"--gpu-architecture=compute_60\"]\n",
    "err, = nvrtc.nvrtcCompileProgram(prog, 2, opts)\n",
    "\n",
    "# Get PTX from compilation\n",
    "err, ptxSize = nvrtc.nvrtcGetPTXSize(prog)\n",
    "ptx = b\" \" * ptxSize\n",
    "err, = nvrtc.nvrtcGetPTX(prog, ptx)\n",
    "\n",
    "# Initialize CUDA Driver API\n",
    "err, = cuda.cuInit(0)\n",
    "\n",
    "# Retrieve handle for device 0\n",
    "err, cuDevice = cuda.cuDeviceGet(0)\n",
    "\n",
    "# Create context\n",
    "err, context = cuda.cuCtxCreate(0, cuDevice)\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "ptx = np.char.array(ptx)\n",
    "# Note: Incompatible --gpu-architecture would be detected here\n",
    "err, module = cuda.cuModuleLoadData(ptx.ctypes.data)\n",
    "ASSERT_DRV(err)\n",
    "err, kernel = cuda.cuModuleGetFunction(module, b\"saxpy\")\n",
    "ASSERT_DRV(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Host and Device vectors are equal\n",
      "result from CUDA (hOut):  [0.8312741 1.719444  2.4221642 ... 1.0741441 1.29529   1.7780536]\n",
      "result from CPU (hZ):  [0.8312741 1.719444  2.4221642 ... 1.0741441 1.29529   1.7780536]\n"
     ]
    }
   ],
   "source": [
    "# a simple vector operation (ax + y = z)\n",
    "# executed on the GPU using CUDA and Python's cuda library\n",
    "\n",
    "NUM_THREADS = 512  # Sets the number of threads within each thread block.\n",
    "NUM_BLOCKS = 32768  # Sets the number of thread blocks within the grid. Threads are organized into blocks, and blocks form a grid, defining the parallel execution structure on the GPU.\n",
    "\n",
    "##### Input Data Preparation\n",
    "# Create a scalar 'a' with value 2.0 (float32)\n",
    "a = np.array([2.0], dtype=np.float32)\n",
    "\n",
    "# Calculate the total number of elements for processing\n",
    "n = np.array(NUM_THREADS * NUM_BLOCKS, dtype=np.uint32)\n",
    "\n",
    "bufferSize = n * a.itemsize  # Calculate the total memory size required for each vector\n",
    "\n",
    "# hX, hY, hOut are host vectors\n",
    "hX = np.random.rand(n).astype(dtype=np.float32)\n",
    "hY = np.random.rand(n).astype(dtype=np.float32)\n",
    "hOut = np.zeros(n).astype(dtype=np.float32)\n",
    "\n",
    "##### Device Memory Allocation\n",
    "err, dXclass = cuda.cuMemAlloc(bufferSize)\n",
    "err, dYclass = cuda.cuMemAlloc(bufferSize)\n",
    "err, dOutclass = cuda.cuMemAlloc(bufferSize)\n",
    "\n",
    "##### Stream Creation\n",
    "err, stream = cuda.cuStreamCreate(0)\n",
    "\n",
    "##### Data Transfer to CUDA Device (Host to Device)\n",
    "err, = cuda.cuMemcpyHtoDAsync(\n",
    "   dXclass, hX.ctypes.data, bufferSize, stream\n",
    ")\n",
    "err, = cuda.cuMemcpyHtoDAsync(\n",
    "   dYclass, hY.ctypes.data, bufferSize, stream\n",
    ")\n",
    "\n",
    "##### Kernel Launch Preparation\n",
    "# Note: Subject to change in a future release\n",
    "dX = np.array([int(dXclass)], dtype=np.uint64)\n",
    "dY = np.array([int(dYclass)], dtype=np.uint64)\n",
    "dOut = np.array([int(dOutclass)], dtype=np.uint64)\n",
    "\n",
    "# Create an array of arguments to pass to the kernel\n",
    "args = [a, dX, dY, dOut, n]\n",
    "# Create an array of arguments to pass to the kernel\n",
    "args = np.array([arg.ctypes.data for arg in args], dtype=np.uint64)\n",
    "\n",
    "##### Launch the CUDA kernel with specified grid and block dimensions\n",
    "err, = cuda.cuLaunchKernel(\n",
    "   kernel,\n",
    "   NUM_BLOCKS,  # grid x dim\n",
    "   1,  # grid y dim\n",
    "   1,  # grid z dim\n",
    "   NUM_THREADS,  # block x dim\n",
    "   1,  # block y dim\n",
    "   1,  # block z dim\n",
    "   0,  # dynamic shared memory\n",
    "   stream,  # stream\n",
    "   args.ctypes.data,  # kernel arguments\n",
    "   0,  # extra (ignore)\n",
    ")\n",
    "\n",
    "# Copy the result 'dOut' from device to host memory asynchronously\n",
    "err, = cuda.cuMemcpyDtoHAsync(\n",
    "   hOut.ctypes.data, dOutclass, bufferSize, stream\n",
    ")\n",
    "\n",
    "# Synchronize the stream, wait for all operations to complete\n",
    "err, = cuda.cuStreamSynchronize(stream)\n",
    "\n",
    "# Assert values are same as host after running kernel\n",
    "hZ = a * hX + hY\n",
    "if not np.allclose(hOut, hZ):\n",
    "   raise ValueError(\"Error outside tolerance for host-device vectors\")\n",
    "else:\n",
    "   print(\"Success: Host and Device vectors are equal\")\n",
    "   print(\"result from CUDA (hOut): \", hOut)\n",
    "   print(\"result from CPU (hZ): \", hZ)\n",
    "\n",
    "##### Clean up resources\n",
    "err, = cuda.cuStreamDestroy(stream)\n",
    "err, = cuda.cuMemFree(dXclass)\n",
    "err, = cuda.cuMemFree(dYclass)\n",
    "err, = cuda.cuMemFree(dOutclass)\n",
    "err, = cuda.cuModuleUnload(module)\n",
    "err, = cuda.cuCtxDestroy(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jae-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
